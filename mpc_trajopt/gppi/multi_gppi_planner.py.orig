import torch
# from .control_priors import diag_Cov, const_ctrl_Cov
from mpc.obstacle_map.map_generator import generate_obstacle_map
from mpc.mp_priors import MP_Prior
from mpc.gppi.mp_priors_multi import Multi_MP_Prior
from mpc.gppi.gp_factor import GPFactor
from mpc.unary_factor import UnaryFactor
import matplotlib.pyplot as plt
import time


class MultiGPPI:

    def __init__(
            self,
            num_particles,
            num_samples,
            traj_len,
            opt_iters,
            dt=None,
            n_dof=None,
            step_size=1.,
            temp=1.,
            start_state=None,
            goal_state=None,
            sigma_start=None,
            sigma_goal=None,
            sigma_goal_init=None,
            sigma_gp=None,
            sigma_gp_init=None,
            w_gp=None,
            w_obst=None,
            seed=0,
            tensor_args=None,
    ):
        if tensor_args is None:
            tensor_args = {'device': torch.device('cuda'), 'dtype': torch.float32}
        self.tensor_args = tensor_args

        torch.manual_seed(seed)

        self.n_dof = n_dof
        self.d_state_opt = 2 * self.n_dof
        self.dt = dt

        self.traj_len = traj_len
        self.num_particles = num_particles
        self.num_samples = num_samples
        self.opt_iters = opt_iters
        self.step_size = step_size
        self.temp = temp
        self.sigma_start = sigma_start
        self.sigma_goal = sigma_goal
        self.sigma_goal_init = sigma_goal_init
        self.sigma_gp = sigma_gp
        self.sigma_gp_init = sigma_gp_init
        self.w_gp = w_gp
        self.w_obst = w_obst
        self.start_state = start_state # position + velocity
        self.goal_state = goal_state # position + velocity

        self._mean = None
        self._weights = None
        self._sample_dist = None

        self.set_prior_factors()
        self.reset(start_state, goal_state)

    def set_prior_factors(self):

        self.start_prior = UnaryFactor(
            self.d_state_opt,
            self.sigma_start,
            self.tensor_args,
        )

        self.goal_prior = UnaryFactor(
            self.d_state_opt,
            self.sigma_goal,
            self.tensor_args,
        )

        self.goal_prior_init = UnaryFactor(
            self.d_state_opt,
            self.sigma_goal_init,
            self.tensor_args,
        )

        self.gp_prior = GPFactor(
            self.n_dof,
            self.sigma_gp,
            self.dt,
            self.traj_len - 1,
            self.tensor_args,
        )

        self.gp_prior_init = GPFactor(
            self.n_dof,
            self.sigma_gp_init,
            self.dt,
            self.traj_len - 1,
            self.tensor_args,
        )

    def get_prior_dist(
            self,
            gp_prior,
            state_init,
            goal_state=None,
    ):

        return MP_Prior(
            self.traj_len - 1,
            self.dt,
            2 * self.n_dof,
            self.n_dof,
            self.start_prior.K,
            gp_prior.Q_inv[0],
            state_init,
            self.tensor_args,
            K_g_inv=self.goal_prior_init.K,
            goal_state=goal_state,
            prior_type='const_velocity',
        )

    def get_sampling_dist(
            self,
            particle_means,
            gp_prior,
            state_init,
            goal_state=None,
    ):

        return Multi_MP_Prior(
            particle_means,
            self.traj_len - 1,
            self.dt,
            2 * self.n_dof,
            self.n_dof,
            self.start_prior.K,
            gp_prior.Q_inv[0],
            state_init,
            self.tensor_args,
            K_g_inv=self.goal_prior.K,
            goal_state=goal_state,
            prior_type='const_velocity',
        )

    def reset(
            self,
            start_state=None,
            goal_state=None,
    ):

        if start_state is not None:
            self.start_state = start_state.clone()

        if goal_state is not None:
            self.goal_state = goal_state.clone()

        # Initialization particles from prior distribution
        self._init_dist = self.get_prior_dist(self.gp_prior_init, self.start_state, self.goal_state)
        self.particle_means = self._init_dist.sample(self.num_particles).to(**self.tensor_args)

        # Sampling distributions
        self._sample_dist = self.get_sampling_dist(self.particle_means, self.gp_prior, self.start_state, self.goal_state)
        self.Sigma_inv = self._sample_dist.Sigma_inv

        self.state_samples = self._sample_dist.sample(self.num_samples).to(**self.tensor_args)

<<<<<<< HEAD
    def _get_costs(self, obst_map):
=======
    def _get_costs(self, observation):
>>>>>>> 18963a740e18029c03198ae456d236a6a321ae6d

        th = self.state_samples.reshape(-1, self.traj_len, self.d_state_opt)

        # Start prior
        err_p = self.start_prior.get_error(th[:, [0]], calc_jacobian=False)
        w_mat = self.start_prior.K
        start_costs = err_p @ w_mat.unsqueeze(0) @ err_p.transpose(1, 2)
        start_costs = start_costs.squeeze()

        # Goal prior
        err_g = self.goal_prior.get_error(th[:, [-1]], calc_jacobian=False)
        w_mat = self.goal_prior.K
        goal_costs = err_g @ w_mat.unsqueeze(0) @ err_g.transpose(1, 2)
        goal_costs = goal_costs.squeeze()

        # GP prior
        err_gp = self.gp_prior.get_error(th, calc_jacobian=False)
        w_mat = self.gp_prior.Q_inv[0] # repeated Q_inv
        w_mat = w_mat.reshape(1, 1, self.d_state_opt, self.d_state_opt)
        gp_costs = err_gp.transpose(2, 3) @ w_mat @ err_gp
        gp_costs = gp_costs.sum(1)
        gp_costs = gp_costs.squeeze()
        gp_costs *= self.w_gp

        costs = start_costs + goal_costs + gp_costs

        # Obstacle cost
        if 'obst_map' in observation:
            obst_cost = observation['obst_map'].get_collisions(th[..., 0:2]) * self.w_obst
            obst_cost = obst_cost.sum(1)
            costs += obst_cost

        costs = costs.reshape(self.num_particles, self.num_samples)

        # Add cost from importance-sampling ratio
        V  = self.state_samples.view(-1, self.num_samples, self.traj_len * self.d_state_opt)  # flatten trajectories
        U = self.particle_means.view(-1, 1, self.traj_len * self.d_state_opt)
        costs += self.temp * (V @ self.Sigma_inv @ U.transpose(1, 2)).squeeze(2)

        return costs

    def sample_and_eval(self, observation):

        # TODO: update prior covariance with new goal location

        # Sample state-trajectory particles
        self.state_samples = self._sample_dist.sample(self.num_samples).to(
            **self.tensor_args)

        # Evaluate costs
<<<<<<< HEAD
        costs = self._get_costs(observation['obst_map'])
=======
        costs = self._get_costs(observation)
>>>>>>> 18963a740e18029c03198ae456d236a6a321ae6d

        position_seq = self.state_samples[..., :self.n_dof]
        velocity_seq = self.state_samples[..., -self.n_dof:]

        return (
            velocity_seq,
            position_seq,
            costs,
        )

    def _update_distribution(self, costs, traj_samples):

        self._weights = torch.softmax( -costs / self.temp, dim=1)
        self._weights = self._weights.reshape(-1, self.num_samples, 1, 1)

        self.particle_means.add_(
            self.step_size * (
                self._weights * (traj_samples - self.particle_means.unsqueeze(1))
            ).sum(1)
        )
        self._sample_dist.set_mean(self.particle_means.view(self.num_particles, -1))

    def optimize(
            self,
            observation={'state': None},
            opt_iters=None,
    ):

        if opt_iters is None:
            opt_iters = self.opt_iters

        for opt_step in range(opt_iters):

            with torch.no_grad():
                (control_samples,
                 state_trajectories,
                 costs,) = self.sample_and_eval(observation)

                self._update_distribution(costs, self.state_samples)

        self._recent_control_samples = control_samples
        self._recent_state_trajectories = state_trajectories
        self._recent_weights = self._weights

        return (
            state_trajectories,
            control_samples,
            costs,
        )

    def _get_traj(self, mode='best'):
        if mode == 'best':
            #TODO: Fix for multi-particles
            particle_ind = self._weights.argmax()
            traj = self.state_samples[particle_ind].clone()
        elif mode == 'mean':
            traj = self._mean.clone()
        else:
            raise ValueError('Unidentified sampling mode in get_next_action')
        return traj

    def get_recent_samples(self):
        return (
            self._recent_control_samples.detach().clone(),
            self._recent_state_trajectories.detach().clone(),
            self._recent_weights.detach().clone(),
        )


if __name__ == "__main__":

    device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')
    # device = 'cpu'
    tensor_args = {'device': device, 'dtype': torch.float32}

    start_q = torch.Tensor([-9, -9]).to(**tensor_args)
    goal_q = torch.Tensor([9, 9]).to(**tensor_args)
    start_state = torch.cat((start_q, torch.zeros(2, **tensor_args)))
    goal_state = torch.cat((goal_q, torch.zeros(2, **tensor_args)))

    seed = 1

    ## Planner - 2D point particle dynamics
    gppi_params = dict(
        num_particles=5,
        num_samples=128,
        traj_len=64,
        dt=0.02,
        n_dof=2,
        opt_iters=1, # Keep this 1 for visualization
        temp=1.,
        start_state=start_state,
        goal_state=goal_state,
        step_size=0.5,
        sigma_start=0.001,
        sigma_goal=0.01,
        sigma_gp=5.,
        sigma_goal_init=0.01,
        sigma_gp_init=50.,
        w_gp=1.e6,
        w_obst=1.e9,
        seed=seed,
        tensor_args=tensor_args,
    )
    planner = MultiGPPI(**gppi_params)

    ## Obstacle map
    # obst_list = [(0, 0, 4, 6)]
    obst_list = []
    cell_size = 0.1
    map_dim = [20, 20]

    obst_params = dict(
        map_dim=map_dim,
        obst_list=obst_list,
        cell_size=cell_size,
        map_type='direct',
        random_gen=True,
        num_obst=10,
        rand_xy_limits=[[-7.5, 7.5], [-7.5, 7.5]],
        rand_shape=[2, 2],
        seed=seed,
        tensor_args=tensor_args,
    )
    obst_map = generate_obstacle_map(**obst_params)

    obs = {
        'state': start_state,
        'goal_state': goal_state,
        'cost_func': None,
        'obst_map': obst_map
    }

    #---------------------------------------------------------------------------
    # Optimize
    opt_iters = 500

    traj_history = []
    for i in range(opt_iters):
        print(i)
        time_start = time.time()
        planner.optimize(obs)
        print(time.time() - time_start)
        controls, trajectories, weights = planner.get_recent_samples()
        traj_history.append(trajectories)
    #---------------------------------------------------------------------------
    # Plotting

    import numpy as np
    x = np.linspace(-10, 10, 200)
    y = np.linspace(-10, 10, 200)

    for iter, trajs in enumerate(traj_history):

        if iter % 250 == 0:
            fig = plt.figure()
            ax = fig.gca()
            cs = ax.contourf(x, y, obst_map.map, 20)
            cbar = fig.colorbar(cs, ax=ax)

            trajs = trajs.cpu().numpy()
            mean_trajs = trajs.mean(1)
            for i in range(trajs.shape[0]):
                for j in range(trajs.shape[1]):
                    ax.plot(trajs[i, j, :, 0], trajs[i, j, :, 1], 'r', alpha=0.15)
            for i in range(trajs.shape[0]):
                ax.plot(mean_trajs[i, :, 0], mean_trajs[i, :, 1], 'b')
            plt.show()
            plt.close('all')
